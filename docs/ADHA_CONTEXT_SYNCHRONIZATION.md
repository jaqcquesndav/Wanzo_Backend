# ADHA Context Synchronisation - Architecture Robuste

## ðŸ“‹ Vue d'ensemble

Synchronisation temps rÃ©el de la base de connaissances entre **admin-service** (PostgreSQL) et **adha-ai-service** (ChromaDB) via Kafka.

## ðŸ”„ Workflow Complet

```
Frontend â†’ Admin Service â†’ PostgreSQL â†’ Kafka â†’ Adha AI Service â†’ ChromaDB
```

### Ã‰tapes du cycle de vie

1. **UPLOAD**: `POST /api/adha-context/upload` â†’ Cloudinary URL (pas encore en BDD)
2. **CREATE**: `POST /api/adha-context/sources` â†’ Sauvegarde PostgreSQL + Kafka event (si indexable)
3. **TOGGLE**: `PATCH /api/adha-context/sources/:id/toggle-active` â†’ Kafka event (si Ã©ligibilitÃ© change)
4. **UPDATE**: `PUT /api/adha-context/sources/:id` â†’ Kafka event (si champs indexation changent)
5. **DELETE**: `DELETE /api/adha-context/sources/:id` â†’ Kafka event (toujours Ã©mis)

## ðŸŽ¯ RÃ¨gles d'Indexation

Un document est **indexable** SI ET SEULEMENT SI :
- âœ… `active === true`
- âœ… `url !== null` (Cloudinary)
- âœ… `(canExpire === false) OU (dateDebut <= NOW <= dateFin)`

## ðŸ“¡ Topics Kafka

| Topic | Ã‰mis quand | Payload clÃ© |
|-------|-----------|-------------|
| `adha.context.created` | Document crÃ©Ã© ET indexable | `shouldIndex: true` |
| `adha.context.updated` | Champs indexation changent | `shouldIndex: bool, changes: []` |
| `adha.context.toggled` | Ã‰ligibilitÃ© indexation change | `shouldIndex: bool, previousState` |
| `adha.context.deleted` | Document supprimÃ© | `id, titre, url` |
| `adha.context.expired` | Job CRON dÃ©tecte expiration | `dateFin, expiredAt` |

## ðŸ›¡ï¸ Protections Anti-Boucles Infinies

### 1. **Idempotence (Consumer)**
```python
message_hash = sha256(f"{event.id}:{event.timestamp}:{event.version}")
if ProcessedMessage.is_already_processed(message_hash):
    return  # Skip duplicate
```
**Protection**: MÃªme message traitÃ© 1 seule fois, mÃªme si reÃ§u multiple fois.

### 2. **Circuit Breaker**
```python
CircuitBreaker(failure_threshold=5, timeout_seconds=60)
States: CLOSED â†’ OPEN (aprÃ¨s 5 erreurs) â†’ HALF_OPEN (test) â†’ CLOSED
```
**Protection**: ArrÃªt automatique aprÃ¨s 5 Ã©checs consÃ©cutifs, reprise aprÃ¨s 60s.

### 3. **Rate Limiting**
```python
RateLimiter(max_per_minute=30)
# Max 30 indexations/minute
```
**Protection**: Limite tokens OpenAI consommÃ©s, Ã©vite explosion des coÃ»ts.

### 4. **Validation Double (Producer + Consumer)**

**Producer (admin-service)** :
```typescript
if (!isIndexable(source)) {
  logger.debug("Document not indexable, no Kafka event");
  return;
}
```

**Consumer (adha-ai-service)** :
```python
is_valid, error = _validate_event(event)
if not is_valid:
  logger.warning(f"Invalid event: {error}")
  return
```

**Protection**: 2 niveaux de validation = impossible d'indexer document invalide.

### 5. **Ã‰mission Conditionnelle**

**CREATE** : Ã‰mettre SI `isIndexable() === true`
**UPDATE** : Ã‰mettre SI `indexationFieldsChanged AND (wasIndexable OR isNowIndexable)`
**TOGGLE** : Ã‰mettre SI `wasIndexable !== isNowIndexable`
**DELETE** : **TOUJOURS** Ã©mettre

**Protection**: Ã‰vÃ©nements Ã©mis UNIQUEMENT si impact sur l'indexation = pas de bruit Kafka.

### 6. **Timeout sur OpÃ©rations**
```python
DOWNLOAD_TIMEOUT_SECONDS = 30
INDEXATION_TIMEOUT_SECONDS = 60
```
**Protection**: Ã‰vite blocage infini sur tÃ©lÃ©chargement/indexation.

### 7. **DÃ©connexion Kafka/OpÃ©rations**
```typescript
try {
  await eventsService.publishAdhaContextCreated(event);
} catch (error) {
  logger.error("Kafka failed but document saved");
  // NE PAS BLOQUER la sauvegarde PostgreSQL
}
```
**Protection**: Ã‰chec Kafka n'empÃªche pas les opÃ©rations CRUD.

## ðŸ“Š MÃ©triques et Monitoring

### Statistiques Consumer
```python
stats = {
  'processed': 0,
  'created': 0,
  'updated': 0,
  'deleted': 0,
  'skipped_duplicate': 0,    # Idempotence
  'skipped_invalid': 0,       # Validation
  'skipped_rate_limit': 0,    # Rate limiting
  'errors': 0,
  'circuit_breaker_trips': 0,
}
```

### Health Check
```python
GET /api/health/adha-context-consumer
{
  "status": "healthy",
  "circuit_breaker": "CLOSED",
  "current_rate": "15/30",
  "stats": {...}
}
```

## ðŸš¨ ScÃ©narios d'Erreur

### ScÃ©nario 1: Cloudinary Injoignable
**SymptÃ´me**: TÃ©lÃ©chargement PDF Ã©choue  
**Protection**: Timeout 30s + Circuit Breaker + Retry DLQ  
**Impact**: Document non indexÃ©, metadata conservÃ©e, retry automatique

### ScÃ©nario 2: OpenAI API Limite Atteinte
**SymptÃ´me**: Embeddings Ã©chouent  
**Protection**: Rate Limiter bloque Ã  30/min  
**Impact**: Messages mis en attente, traitement diffÃ©rÃ©

### ScÃ©nario 3: ChromaDB Corruption
**SymptÃ´me**: Collection inaccessible  
**Protection**: Circuit Breaker ouvre aprÃ¨s 5 Ã©checs  
**Impact**: Consumer s'arrÃªte, alerte Ã©mise, admin intervient

### ScÃ©nario 4: Kafka Consumer Lag
**SymptÃ´me**: 1000+ messages en attente  
**Protection**: Rate Limiter + Processing Time monitoring  
**Impact**: Traitement ralenti mais stable, pas d'explosion

### ScÃ©nario 5: Document Ã‰norme (500 pages)
**SymptÃ´me**: Indexation trÃ¨s longue  
**Protection**: Timeout 60s + chunking 1000 chars  
**Impact**: Timeout â†’ DLQ â†’ traitement manuel

## ðŸ”§ Configuration RecommandÃ©e

### Environment Variables
```bash
# Admin Service
USE_KAFKA=true
KAFKA_BROKERS=kafka:9092
ADHA_CONTEXT_EVENT_VERSION=1.0.0

# Adha AI Service
OPENAI_API_KEY=sk-...
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
ADHA_CONTEXT_MAX_RATE=30
CIRCUIT_BREAKER_THRESHOLD=5
DOWNLOAD_TIMEOUT=30
INDEXATION_TIMEOUT=60
```

### Kafka Topics Configuration
```yaml
topics:
  adha.context.created:
    partitions: 3
    replication: 2
    retention.ms: 604800000  # 7 days
  
  adha.context.updated:
    partitions: 3
    replication: 2
    retention.ms: 604800000
  
  adha.context.deleted:
    partitions: 1
    replication: 2
    retention.ms: 2592000000  # 30 days (audit)
```

## ðŸ“ Exemples de Payloads

### Created Event
```json
{
  "id": "uuid-123",
  "titre": "Guide Fiscal RDC 2025",
  "description": "...",
  "type": "guide",
  "url": "https://cloudinary.com/...",
  "active": true,
  "canExpire": true,
  "dateDebut": "2025-01-01T00:00:00Z",
  "dateFin": "2025-12-31T23:59:59Z",
  "shouldIndex": true,
  "timestamp": "2025-11-21T10:30:00Z",
  "version": "1.0.0",
  "metadata": {
    "createdAt": "2025-11-21T10:30:00Z",
    "sourceService": "admin-service"
  }
}
```

### Updated Event
```json
{
  "id": "uuid-123",
  "titre": "Guide Fiscal RDC 2025 (Mis Ã  jour)",
  "shouldIndex": false,
  "previouslyIndexable": true,
  "changes": ["titre", "active", "dateFin"],
  "timestamp": "2025-11-21T11:00:00Z",
  "version": "1.0.0"
}
```

### Toggled Event
```json
{
  "id": "uuid-123",
  "titre": "Guide Fiscal RDC 2025",
  "active": false,
  "shouldIndex": false,
  "previousState": {
    "active": true,
    "wasIndexable": true
  },
  "timestamp": "2025-11-21T11:30:00Z",
  "version": "1.0.0"
}
```

## ðŸŽ“ Best Practices

### 1. **Toujours utiliser `isIndexable()`**
Ne jamais indexer sans vÃ©rifier l'Ã©ligibilitÃ©.

### 2. **Logger avec contexte**
```typescript
logger.log(`âœ… Kafka event emitted: adha.context.created for ${id} (${titre})`);
logger.debug(`â­ï¸ Document ${id} not indexable (active=${active}). No event.`);
```

### 3. **GÃ©rer les Ã©checs Kafka gracieusement**
Ne jamais bloquer les opÃ©rations CRUD si Kafka Ã©choue.

### 4. **Monitorer les mÃ©triques**
- `circuit_breaker_state`: doit Ãªtre `CLOSED`
- `skipped_rate_limit`: si >100, augmenter le rate limit
- `errors`: si >10%, investiguer

### 5. **Tester les scÃ©narios d'erreur**
- Document sans URL â†’ ne doit pas indexer
- Document expirÃ© â†’ doit retirer de l'index
- Toggle active=false â†’ doit retirer immÃ©diatement
- Cloudinary down â†’ doit retry puis DLQ

## ðŸš€ DÃ©ploiement

### 1. DÃ©ployer shared package
```bash
cd packages/shared
yarn build
```

### 2. RedÃ©marrer admin-service
```bash
docker-compose restart admin-service
```

### 3. RedÃ©marrer adha-ai-service consumers
```bash
docker-compose restart adha-ai-service
# Ou manuellement:
python start_consumers.py
```

### 4. VÃ©rifier les logs
```bash
# Admin service
docker-compose logs -f admin-service | grep "adha.context"

# Adha AI service
docker-compose logs -f adha-ai-service | grep "ADHA Context"
```

### 5. Health check
```bash
curl http://localhost:8000/api/health/adha-context-consumer
```

## ðŸ“š Ressources

- **Code TypeScript**: `packages/shared/src/events/adha-context-events.ts`
- **Producer**: `apps/admin-service/src/modules/adha-context/services/adha-context.service.ts`
- **Consumer**: `apps/Adha-ai-service/api/kafka/adha_context_consumer.py`
- **Ingestor**: `apps/Adha-ai-service/agents/logic/adha_context_ingest.py`
- **Start Script**: `apps/Adha-ai-service/start_consumers.py`

---

**Version**: 1.0.0  
**Date**: 2025-11-21  
**Auteur**: GitHub Copilot  
**Status**: âœ… Production Ready
