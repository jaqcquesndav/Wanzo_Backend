import uuid
import json
from datetime import datetime
import os
from typing import Dict, Any, List, Optional
import re

import tiktoken
from openai import OpenAI
from django.db.models import Q
from django.contrib.auth.models import User
from django.conf import settings
import chromadb.utils.embedding_functions as embedding_functions

from api.models import JournalEntry, ChatConversation, ChatMessage
from agents.vector_databases.chromadb_connector import ChromaDBConnector
from agents.logic.retriever_agent import RetrieverAgent
from agents.utils.llm_tool_system import LLMToolSystem
from agents.core.adha_identity import ADHAIdentity

class HistoryAgent:
    """
    Agent responsable de la gestion de l'historique des √©critures et des conversations.
    Permet d'interroger l'historique comptable et de maintenir le contexte des conversations.
    """
    def __init__(self, user_id=None, company_id=None, institution_id=None, customer_type='sme'):
        self.client = OpenAI()
        self.user_id = user_id
        self.company_id = company_id
        self.institution_id = institution_id
        self.customer_type = customer_type
        self.conversation_storage = {}  # Stockage en m√©moire (fallback)
        
        # Initialiser le syst√®me d'outils LLM
        self.tool_system = LLMToolSystem()
        
        # Initialisation de la base de donn√©es vectorielle pour l'historique
        try:
            # Utiliser le chemin d√©fini dans settings
            embeddings_path = os.path.join(settings.BASE_DIR, 'data', 'embeddings')
            os.makedirs(embeddings_path, exist_ok=True)
            print(f"Using embeddings path: {embeddings_path}")
            
            # Initialize OpenAI embedding function (read from environment)
            openai_api_key = os.environ.get("OPENAI_API_KEY")
            openai_model = os.environ.get("OPENAI_EMBEDDING_MODEL", "text-embedding-3-small")
            
            if not openai_api_key:
                raise ValueError("OPENAI_API_KEY environment variable is required for history agent")
            
            self.openai_ef = embedding_functions.OpenAIEmbeddingFunction(
                api_key=openai_api_key,
                model_name=openai_model
            )
            print(f"History agent: OpenAI embeddings initialized with model: {openai_model}")
            
            self.vector_db = ChromaDBConnector(
                persist_directory=embeddings_path,
                embedding_function=self.openai_ef
            )
            # Collection sp√©cifique pour les √©critures comptables
            self.entries_collection = self.vector_db.get_or_create_collection(
                name="journal_entries",
                embedding_function=self.openai_ef
            )
            # Collection s√©par√©e pour l'historique des conversations
            self.chats_collection = self.vector_db.get_or_create_collection(
                name="chat_history",
                embedding_function=self.openai_ef
            )
            
            print("OpenAI embeddings initialized successfully for history agent")
        except Exception as e:
            print(f"Erreur lors de l'initialisation de la base vectorielle: {e}")
            self.vector_db = None
            self.entries_collection = None
            self.chats_collection = None
            self.openai_ef = None
    
    def add_entry(self, entry, source_data=None, source_type="manual"):
        """
        Ajoute une √©criture comptable √† l'historique.
        
        Args:
            entry: L'√©criture comptable √† ajouter
            source_data: Les donn√©es source qui ont permis de g√©n√©rer l'√©criture (extrait OCR ou prompt)
            source_type: Le type de source ("document" ou "prompt")
        """
        try:
            # Indexer l'entr√©e dans la base vectorielle
            entry_text = f"{entry.get('description', '')} - {entry.get('date', '')} - {entry.get('piece_reference', '')}"
            
            # Enrichir le texte de l'entr√©e avec les donn√©es source pour une meilleure recherche s√©mantique
            if source_data:
                if isinstance(source_data, dict) and 'full_text' in source_data:
                    # Pour un document OCR, utiliser le texte extrait
                    entry_text += f" Source: {source_data.get('full_text', '')[:500]}"
                elif isinstance(source_data, str):
                    # Pour un prompt en langage naturel
                    entry_text += f" Source: {source_data[:500]}"
            
            # Extraire des d√©tails pertinents des d√©bits et cr√©dits
            for debit in entry.get('debit', []):
                entry_text += f" D√©bit: {debit.get('compte', '')} {debit.get('montant', '')} {debit.get('libelle', '')}"
            for credit in entry.get('credit', []):
                entry_text += f" Cr√©dit: {credit.get('compte', '')} {credit.get('montant', '')} {credit.get('libelle', '')}"
            
            # G√©n√©rer un ID unique pour l'entr√©e
            entry_id = f"entry_{uuid.uuid4()}"
            
            # V√©rifier si le module d'embedding est disponible
            if self.openai_ef and self.entries_collection:
                # Cr√©er l'embedding avec OpenAI API
                # ChromaDB with OpenAI embedding function will handle this automatically
                # We just need to provide the text, no manual embedding needed
                embedding = None  # Will be generated automatically by ChromaDB
                
                # Pr√©parer les m√©tadonn√©es incluant les donn√©es source avec isolation stricte
                metadata = {
                    "date": entry.get("date", ""),
                    "description": entry.get("description", ""),
                    "piece_reference": entry.get("piece_reference", ""),
                    "type": "journal_entry",
                    "entry_data": json.dumps(entry),
                    "user_id": str(self.user_id) if self.user_id else "global",
                    "company_id": str(self.company_id) if self.company_id else None,
                    "institution_id": str(self.institution_id) if self.institution_id else None,
                    "customer_type": self.customer_type
                }
                
                # Ajouter les donn√©es source si disponibles
                if source_data:
                    if isinstance(source_data, dict):
                        # Pour les documents, stocker des extraits pertinents
                        metadata["source_extract"] = source_data.get("full_text", "")[:1000] if "full_text" in source_data else ""
                    else:
                        # Pour les prompts
                        metadata["source_prompt"] = source_data[:1000] if source_data else ""
                        
                    metadata["source_type"] = source_type
                
                # Stocker dans la base vectorielle
                # With OpenAI embedding function, ChromaDB generates embeddings automatically
                self.entries_collection.add(
                    ids=[entry_id],
                    documents=[entry_text],
                    metadatas=[metadata]
                )
            
            # Sauvegarder √©galement en base de donn√©es relationnelle
            try:
                # Pr√©parer les donn√©es source √† sauvegarder
                source_data_to_save = {}
                if source_data:
                    if isinstance(source_data, dict):
                        # Pour les documents, extraire les champs pertinents
                        source_data_to_save = {
                            "document_type": source_data.get("document_type", "unknown"),
                            "excerpt": source_data.get("full_text", "")[:2000] if "full_text" in source_data else "",
                            "confidence": source_data.get("confidence", 0)
                        }
                    else:
                        # Pour les prompts
                        source_data_to_save = {"prompt_text": source_data}
                
                journal_entry = JournalEntry(
                    date=datetime.strptime(entry["date"], "%d/%m/%Y").date() if "date" in entry else datetime.now().date(),
                    piece_reference=entry.get("piece_reference", ""),
                    description=entry.get("description", ""),
                    debit_data=entry.get("debit", []),
                    credit_data=entry.get("credit", []),
                    source_data=source_data_to_save,
                    source_type=source_type,
                    created_by_id=self.user_id  # Associer l'utilisateur
                )
                journal_entry.save()
                print(f"Entry saved to database: {journal_entry.id}")
            except Exception as db_error:
                print(f"Error saving entry to database: {db_error}")
            
            return {"status": "success", "message": "√âcriture ajout√©e √† l'historique", "entry_id": entry_id}
        
        except Exception as e:
            print(f"Erreur lors de l'ajout √† l'historique: {e}")
            return {"status": "error", "message": f"Erreur: {str(e)}"}

    def _get_or_create_conversation(self, conversation_id=None):
        """
        R√©cup√®re ou cr√©e une conversation pour le chat.
        """
        try:
            if conversation_id:
                try:
                    # Tenter de r√©cup√©rer une conversation existante
                    conversation = ChatConversation.objects.get(
                        conversation_id=conversation_id, 
                        user_id=self.user_id
                    )
                    return conversation
                except ChatConversation.DoesNotExist:
                    pass
            
            # Cr√©er une nouvelle conversation si aucune n'existe ou n'est sp√©cifi√©e
            new_id = f"conv_{uuid.uuid4()}"
            user = User.objects.get(id=self.user_id)
            conversation = ChatConversation(conversation_id=new_id, user=user)
            conversation.save()
            return conversation
        except Exception as e:
            print(f"Erreur lors de la cr√©ation de la conversation: {e}")
            # Cr√©er un identifiant de conversation m√™me en cas d'√©chec pour pouvoir continuer
            return {"conversation_id": f"error_conv_{uuid.uuid4()}", "error": str(e)}

    def _get_conversation_history(self, conversation_id):
        """
        R√©cup√®re l'historique des messages d'une conversation sp√©cifique.
        """
        try:
            # R√©cup√©rer la conversation
            conversation = None
            try:
                conversation = ChatConversation.objects.get(conversation_id=conversation_id)
            except ChatConversation.DoesNotExist:
                print(f"Conversation {conversation_id} not found in database")
                return []
            except Exception as db_error:
                print(f"Database error retrieving conversation: {db_error}")
                # Fallback au stockage en m√©moire si disponible
                if conversation_id in self.conversation_storage:
                    return self.conversation_storage[conversation_id]
                return []
                
            # R√©cup√©rer les messages de cette conversation
            messages = ChatMessage.objects.filter(conversation=conversation).order_by('timestamp')
            
            # Convertir en format attendu pour le contexte
            history = []
            for msg in messages:
                history.append({
                    "role": "user" if msg.is_user else "assistant",
                    "content": msg.content,
                    "is_user": msg.is_user,
                    "timestamp": msg.timestamp.isoformat() if msg.timestamp else None,
                    "relevant_entries": msg.relevant_entries
                })
                
            print(f"Retrieved {len(history)} messages from database for conversation {conversation_id}")
            return history
            
        except Exception as e:
            print(f"Error retrieving conversation history: {e}")
            return []

    def _get_relevant_accounting_entries(self, query, max_entries=5):
        """
        R√©cup√®re les √©critures comptables pertinentes par rapport √† une requ√™te,
        ind√©pendamment de la conversation en cours.
        """
        try:
            # 1. D'abord, essayons de trouver des √©critures via la recherche vectorielle
            relevant_entries = []
            
            if self.entries_collection and self.openai_ef:
                # With OpenAI embedding function, we can query directly with text
                # ChromaDB will generate embeddings automatically
                
                # Filtrage strict par isolation - CRITIQUE pour la s√©curit√©
                where_clause = {
                    "$and": [
                        {"user_id": str(self.user_id)} if self.user_id else {"user_id": {"$ne": None}},
                    ]
                }
                
                # Ajouter filtrage par company_id ou institution_id selon le type d'utilisateur
                if self.customer_type == 'institution' and self.institution_id:
                    where_clause["$and"].append({"institution_id": str(self.institution_id)})
                elif self.company_id:
                    where_clause["$and"].append({"company_id": str(self.company_id)})
                
                # Rechercher dans la base vectorielle avec isolation stricte
                # With OpenAI embedding function, use query_texts instead of query_embeddings
                results = self.entries_collection.query(
                    query_texts=[query],
                    n_results=max_entries,
                    where=where_clause,
                    include=['metadatas', 'documents', 'distances']
                )
                
                # R√©cup√©rer les entr√©es pertinentes
                if results and results.get('ids') and len(results['ids'][0]) > 0:
                    for i, entry_id in enumerate(results['ids'][0]):
                        try:
                            metadata = results['metadatas'][0][i] if results.get('metadatas') else {}
                            
                            # V√©rifier si l'entr√©e appartient √† l'utilisateur actuel ou est globale
                            # Si pas d'utilisateur sp√©cifi√©, on prend toutes les entr√©es
                            if not self.user_id or not metadata.get('user_id') or \
                               str(metadata.get('user_id')) == str(self.user_id) or \
                               metadata.get('user_id') == "global":
                                
                                # Extraire les donn√©es de l'√©criture
                                if 'entry_data' in metadata:
                                    entry_data = json.loads(metadata['entry_data'])
                                    
                                    # Ajouter les donn√©es source √† l'entr√©e pour enrichir le contexte
                                    if 'source_extract' in metadata and metadata['source_extract']:
                                        entry_data['source_extract'] = metadata['source_extract']
                                    if 'source_prompt' in metadata and metadata['source_prompt']:
                                        entry_data['source_prompt'] = metadata['source_prompt']
                                    if 'source_type' in metadata:
                                        entry_data['source_type'] = metadata['source_type']
                                        
                                    relevant_entries.append(entry_data)
                        except Exception as e:
                            print(f"Erreur lors de la r√©cup√©ration de l'entr√©e {entry_id}: {e}")
                           
            # 2. Si aucun r√©sultat ou peu de r√©sultats, compl√©ter avec des requ√™tes sur la base relationnelle
            if len(relevant_entries) < max_entries:
                db_entries = self._fallback_entries_search(query, max_entries - len(relevant_entries))
                
                # Ajouter uniquement les entr√©es qui ne sont pas d√©j√† pr√©sentes
                for entry in db_entries:
                    if not any(e.get('piece_reference') == entry.get('piece_reference') for e in relevant_entries):
                        relevant_entries.append(entry)
                        
            print(f"Found {len(relevant_entries)} relevant entries for query: {query}")
            return relevant_entries
            
        except Exception as e:
            print(f"Erreur lors de la recherche d'√©critures comptables: {e}")
            return self._fallback_entries_search(query, max_entries)

    def _fallback_entries_search(self, query, max_entries=5):
        """
        M√©thode de secours pour rechercher des √©critures via la base de donn√©es relationnelle.
        Accessible ind√©pendamment de la conversation en cours.
        """
        try:
            # Extraire des mots-cl√©s potentiels de la requ√™te
            keywords = self._extract_keywords(query)
            
            # Construire une requ√™te pour la base de donn√©es
            db_query = Q()
            
            # Si des mots-cl√©s ont √©t√© extraits, les utiliser pour filtrer
            if keywords:
                for keyword in keywords:
                    if len(keyword) > 2:  # Ignorer les mots tr√®s courts
                        db_query |= Q(description__icontains=keyword)
                        db_query |= Q(piece_reference__icontains=keyword)
                        db_query |= Q(debit_data__contains=[{"libelle": keyword}])
                        db_query |= Q(credit_data__contains=[{"libelle": keyword}])
                        # Pour les num√©ros de compte
                        if keyword.isdigit():
                            db_query |= Q(debit_data__contains=[{"compte": keyword}])
                            db_query |= Q(credit_data__contains=[{"compte": keyword}])
            
            # Si l'utilisateur est sp√©cifi√©, filtrer par utilisateur ou entr√©es publiques
            if self.user_id:
                user_filter = Q(created_by_id=self.user_id) | Q(created_by__isnull=True)
                db_query = db_query & user_filter if db_query else user_filter
            
            # Ex√©cuter la requ√™te
            if db_query:
                db_entries = JournalEntry.objects.filter(db_query).order_by('-date')[:max_entries]
            else:
                # Si pas de mots-cl√©s ou requ√™te vide, prendre les entr√©es les plus r√©centes
                if self.user_id:
                    db_entries = JournalEntry.objects.filter(
                        Q(created_by_id=self.user_id) | Q(created_by__isnull=True)
                    ).order_by('-date')[:max_entries]
                else:
                    db_entries = JournalEntry.objects.all().order_by('-date')[:max_entries]
            
            # Convertir les entr√©es au format attendu
            formatted_entries = []
            for entry in db_entries:
                formatted_entries.append({
                    "id": entry.id,
                    "date": entry.date.strftime("%d/%m/%Y"),
                    "piece_reference": entry.piece_reference,
                    "description": entry.description,
                    "debit": entry.debit_data,
                    "credit": entry.credit_data,
                    "journal": "JO"  # Journal par d√©faut
                })
                
            return formatted_entries
            
        except Exception as e:
            print(f"Erreur lors de la recherche d'√©critures dans la base de donn√©es: {e}")
            return []

    def chat(self, prompt, conversation_id=None, user_context=None):
        """
        Discute avec l'agent en utilisant l'historique des √©critures comptables comme contexte.
        
        Args:
            prompt (str): La question ou l'instruction de l'utilisateur
            conversation_id (str, optional): L'identifiant de la conversation pour le suivi du contexte
            user_context (dict, optional): Contexte utilisateur pour personnaliser la r√©ponse
            
        Returns:
            dict: La r√©ponse de l'agent
        """
        debug_info = {"etape": "debut_chat", "prompt": prompt, "conversation_id": conversation_id}
        print(f"HistoryAgent.chat: prompt={prompt}, conversation_id={conversation_id}")
        
        try:
            # Format du nom d'utilisateur pour la personnalisation
            user_greeting = ""
            company_context = ""
            
            if user_context:
                user_name = user_context.get('name')
                company_name = user_context.get('company')
                is_new_conversation = user_context.get('is_new_conversation', False)
                
                if user_name:
                    user_greeting = f"Bonjour {user_name}, "
                    
                if company_name:
                    company_context = f"En tant que comptable de {company_name}, "
                    
                # Ajouter une salutation sp√©ciale pour les nouvelles conversations
                if is_new_conversation:
                    user_greeting += "ravi de vous assister aujourd'hui! "
            
            # R√©cup√©rer les messages de conversation pr√©c√©dents pour le contexte
            conversation_history = []
            if conversation_id:
                try:
                    conversation_history = self._get_conversation_history(conversation_id)
                    debug_info["conversation_history_length"] = len(conversation_history)
                except Exception as history_error:
                    print(f"Erreur lors de la r√©cup√©ration de l'historique de conversation: {history_error}")
                    debug_info["conversation_history_error"] = str(history_error)
            
            # Trouver les √©critures pertinentes depuis l'historique global (ind√©pendamment de la conversation)
            relevant_entries = self._get_relevant_accounting_entries(prompt, max_entries=5)
            debug_info["relevant_entries_count"] = len(relevant_entries)
            
            # Pr√©parer un prompt enrichi avec identit√© √©thique ADHA, contexte conversation et √©critures pertinentes
            base_identity = ADHAIdentity.get_system_prompt(mode="chat")
            
            system_prompt = f"""{base_identity}

## INSTRUCTIONS TECHNIQUES COMPTABLES

{company_context}R√©pondez pr√©cis√©ment aux questions en utilisant les donn√©es des √©critures comptables fournies si pertinent.

IMPORTANT - Outils de calcul disponibles:
Vous avez acc√®s √† des outils de calcul pr√©cis. Utilisez-les OBLIGATOIREMENT pour tous les calculs (TVA, arithm√©tique, soldes comptables, pourcentages, etc.).
Ne faites JAMAIS de calculs manuels - utilisez toujours les outils appropri√©s pour garantir la pr√©cision.

R√®gles importantes:
1. Soyez pr√©cis et factuel dans vos r√©ponses en vous basant sur les donn√©es comptables.
2. Restez professionnel tout en √©tant cordial et naturel dans le dialogue en fran√ßais.
3. Identifiez les √©critures pertinentes pour r√©pondre √† la question.
4. UTILISEZ LES OUTILS DE CALCUL pour toute op√©ration arithm√©tique ou comptable.
5. Expliquez toujours votre raisonnement de mani√®re p√©dagogique.
6. Pr√©sentez les r√©sultats de calculs de mani√®re claire et format√©e.

Pour toute question sur un compte, utilisez le format SYSCOHADA: Num√©ro + Nom du compte (ex: "512 - Banque").
"""
            
            # Construire le contexte de conversation pour l'API
            messages = [{"role": "system", "content": system_prompt}]
            
            # Limiter l'historique aux 10 derniers √©changes pour √©viter des contextes trop longs
            recent_history = conversation_history[-10:] if len(conversation_history) > 10 else conversation_history
            
            # Ajouter l'historique r√©cent de conversation
            for message in recent_history:
                role = "user" if message.get("is_user", False) else "assistant"
                messages.append({"role": role, "content": message.get("content", "")})
            
            # Ajouter le contexte des √©critures pertinentes (AVANT la demande actuelle)
            if relevant_entries:
                entries_context = "\n\nVoici les √©critures comptables pertinentes de la base de donn√©es:\n"
                
                for i, entry in enumerate(relevant_entries[:5]):  # Limiter √† 5 √©critures pour le contexte
                    entries_context += f"\n√âcriture {i+1}:\n"
                    entries_context += f"Date: {entry.get('date', 'N/A')}\n"
                    entries_context += f"Description: {entry.get('description', 'N/A')}\n"
                    entries_context += f"R√©f√©rence: {entry.get('piece_reference', 'N/A')}\n"
                    
                    debits = entry.get('debit', [])
                    entries_context += "D√©bit:\n"
                    for debit in debits:
                        entries_context += f"- {debit.get('compte', 'N/A')}: {debit.get('montant', 0)} ({debit.get('libelle', 'N/A')})\n"
                    
                    credits = entry.get('credit', [])
                    entries_context += "Cr√©dit:\n"
                    for credit in credits:
                        entries_context += f"- {credit.get('compte', 'N/A')}: {credit.get('montant', 0)} ({credit.get('libelle', 'N/A')})\n"
                    
                    entries_context += "\n"
                
                # Ajouter le contexte des √©critures en tant qu'assistant
                messages.append({
                    "role": "assistant", 
                    "content": "Pour r√©pondre √† votre question, j'ai consult√© l'historique des √©critures comptables. "
                               "Voici les √©critures pertinentes que j'ai trouv√©es:"
                })
                messages.append({"role": "assistant", "content": entries_context})
            
            # --- Ajout RAG documentaire ---
            try:
                retriever_agent = RetrieverAgent()
                rag_docs = retriever_agent.retrieve_adha_context(prompt, top_k=3)
                if rag_docs:
                    rag_context = '\n\n'.join(rag_docs)
                    messages.append({
                        "role": "assistant",
                        "content": "Voici des extraits de documents pertinents issus de la base documentaire¬†:"
                    })
                    messages.append({
                        "role": "assistant",
                        "content": rag_context
                    })
            except Exception as rag_e:
                print(f"Erreur RAG documentaire: {rag_e}")
        
            # Ajouter la demande actuelle
            messages.append({"role": "user", "content": prompt})
            
            debug_info["full_prompt"] = messages
            
            # Appeler le mod√®le de langage avec les outils de calcul
            try:
                response = self.client.chat.completions.create(
                    model="gpt-4o-2024-08-06",
                    messages=messages,
                    tools=self.tool_system.get_openai_function_definitions(),
                    tool_choice="auto",  # Le LLM d√©cide quand utiliser les outils
                    temperature=0.7,
                    max_tokens=1500,
                    top_p=1.0,
                    frequency_penalty=0.0,
                    presence_penalty=0.0
                )
                
                response_message = response.choices[0].message
                
                # Traiter les appels d'outils si pr√©sents
                if response_message.tool_calls:
                    # Le LLM veut utiliser des outils de calcul
                    messages.append(response_message)
                    
                    # Ex√©cuter chaque outil demand√©
                    for tool_call in response_message.tool_calls:
                        function_name = tool_call.function.name
                        function_args = json.loads(tool_call.function.arguments)
                        
                        # Ex√©cuter l'outil
                        tool_result = self.tool_system.execute_tool(function_name, function_args)
                        
                        # Ajouter le r√©sultat du calcul au contexte
                        tool_message = {
                            "tool_call_id": tool_call.id,
                            "role": "tool",
                            "name": function_name,
                            "content": json.dumps(tool_result)
                        }
                        messages.append(tool_message)
                    
                    # Demander au LLM de formuler une r√©ponse finale avec les r√©sultats des calculs
                    final_response = self.client.chat.completions.create(
                        model="gpt-4o-2024-08-06",
                        messages=messages,
                        temperature=0.7,
                        max_tokens=1500
                    )
                    
                    ai_response = final_response.choices[0].message.content
                else:
                    # Pas d'outils utilis√©s, r√©ponse directe
                    ai_response = response_message.content
                
                # Personnaliser la r√©ponse avec le nom d'utilisateur si disponible
                if user_greeting and not any(greeting in ai_response.lower() for greeting in ["bonjour", "salut", "hello"]):
                    ai_response = f"{user_greeting}{ai_response}"
                
                debug_info["etape"] = "complet√©"
            except Exception as api_error:
                print(f"Erreur lors de l'appel √† l'API OpenAI: {api_error}")
                debug_info["etape"] = "erreur_api"
                debug_info["api_error"] = str(api_error)
                return {"erreur": f"Erreur lors de l'appel √† l'API: {str(api_error)}", "debug_info": debug_info}
            
            # G√©n√©rer un ID de message pour le message courant
            message_id = str(uuid.uuid4())
            
            # Sauvegarder la conversation pour de futures interactions
            conversation = None
            if conversation_id:
                try:
                    # R√©cup√©rer ou cr√©er la conversation
                    conversation = self._get_or_create_conversation(conversation_id)
                    if not isinstance(conversation, dict):  # V√©rifier qu'on a bien un objet conversation et pas un dict d'erreur
                        # Sauvegarder le message de l'utilisateur
                        user_msg = ChatMessage(
                            message_id=str(uuid.uuid4()),
                            conversation=conversation,
                            is_user=True,
                            content=prompt,
                            relevant_entries=[]
                        )
                        user_msg.save()
                        
                        # Sauvegarder la r√©ponse de l'IA avec les √©critures pertinentes
                        ai_msg = ChatMessage(
                            message_id=message_id,
                            conversation=conversation,
                            is_user=False,
                            content=ai_response,
                            relevant_entries=[self._format_entry_for_response(entry) for entry in relevant_entries[:3]]
                        )
                        ai_msg.save()
                        
                        # Mise √† jour de la date de derni√®re activit√© de la conversation
                        conversation.updated_at = datetime.now()
                        conversation.save()
                except Exception as save_error:
                    print(f"Erreur lors de la sauvegarde de la conversation: {save_error}")
                    debug_info["save_error"] = str(save_error)
            
            # Pr√©parer la r√©ponse
            result = {
                "response": ai_response,
                "conversation_id": conversation_id if conversation_id else (conversation.conversation_id if conversation else None),
                "message_id": message_id,
                "conversation_context": [{
                    "role": "user" if msg.get("is_user", False) else "assistant",
                    "content": self._truncate_text(msg.get("content", ""), 100)
                } for msg in recent_history],
                "relevant_entries": [self._format_entry_for_response(entry) for entry in relevant_entries[:3]],
                "debug_info": debug_info
            }
            
            return result
        except Exception as e:
            print(f"Erreur dans HistoryAgent.chat: {e}")
            debug_info["etape"] = "erreur"
            debug_info["error"] = str(e)
            return {"erreur": f"Erreur lors du traitement: {str(e)}", "debug_info": debug_info}

    def chat_stream(self, prompt, conversation_id=None, user_context=None):
        """
        Version streaming de chat avec support des outils de calcul.
        Le LLM peut appeler des outils pendant le streaming.
        """
        debug_info = {"etape": "debut_chat_stream", "prompt": prompt, "conversation_id": conversation_id}
        try:
            user_greeting = ""
            company_context = ""
            if user_context:
                user_name = user_context.get('name')
                company_name = user_context.get('company')
                is_new_conversation = user_context.get('is_new_conversation', False)
                if user_name:
                    user_greeting = f"Bonjour {user_name}, "
                if company_name:
                    company_context = f"En tant que comptable de {company_name}, "
                if is_new_conversation:
                    user_greeting += "ravi de vous assister aujourd'hui! "
            
            conversation_history = []
            if conversation_id:
                try:
                    conversation_history = self._get_conversation_history(conversation_id)
                except Exception as history_error:
                    debug_info["conversation_history_error"] = str(history_error)
            
            relevant_entries = self._get_relevant_accounting_entries(prompt, max_entries=5)
            
            base_identity = ADHAIdentity.get_system_prompt(mode="chat")
            
            system_prompt = f"""{base_identity}

## INSTRUCTIONS TECHNIQUES COMPTABLES

{company_context}R√©pondez pr√©cis√©ment aux questions en utilisant les donn√©es des √©critures comptables fournies si pertinent.

IMPORTANT - Outils de calcul disponibles:
Vous avez acc√®s √† des outils de calcul pr√©cis. Utilisez-les OBLIGATOIREMENT pour tous les calculs (TVA, arithm√©tique, soldes comptables, pourcentages, etc.).
Ne faites JAMAIS de calculs manuels - utilisez toujours les outils appropri√©s pour garantir la pr√©cision.

R√®gles importantes:
1. Soyez pr√©cis et factuel dans vos r√©ponses en vous basant sur les donn√©es comptables.
2. Restez professionnel tout en √©tant cordial et naturel dans le dialogue en fran√ßais.
3. Identifiez les √©critures pertinentes pour r√©pondre √† la question.
4. UTILISEZ LES OUTILS DE CALCUL pour toute op√©ration arithm√©tique ou comptable.
5. Expliquez toujours votre raisonnement de mani√®re p√©dagogique.
6. Pr√©sentez les r√©sultats de calculs de mani√®re claire et format√©e.

Pour toute question sur un compte, utilisez le format SYSCOHADA: Num√©ro + Nom du compte (ex: "512 - Banque").
"""
            
            messages = [{"role": "system", "content": system_prompt}]
            
            # Ajouter l'historique et contexte
            recent_history = conversation_history[-10:] if len(conversation_history) > 10 else conversation_history
            for message in recent_history:
                role = "user" if message.get("is_user", False) else "assistant"
                messages.append({"role": role, "content": message.get("content", "")})
            
            # Ajouter le contexte des √©critures pertinentes
            if relevant_entries:
                entries_context = "\\n\\nVoici les √©critures comptables pertinentes de la base de donn√©es:\\n"
                for i, entry in enumerate(relevant_entries[:5]):
                    entries_context += f"\\n√âcriture {i+1}:\\n"
                    entries_context += f"Date: {entry.get('date', 'N/A')}\\n"
                    entries_context += f"Description: {entry.get('description', 'N/A')}\\n"
                    entries_context += f"R√©f√©rence: {entry.get('piece_reference', 'N/A')}\\n"
                    
                    debits = entry.get('debit', [])
                    entries_context += "D√©bit:\\n"
                    for debit in debits:
                        entries_context += f"- {debit.get('compte', 'N/A')}: {debit.get('montant', 0)} ({debit.get('libelle', 'N/A')})\\n"
                    
                    credits = entry.get('credit', [])
                    entries_context += "Cr√©dit:\\n"
                    for credit in credits:
                        entries_context += f"- {credit.get('compte', 'N/A')}: {credit.get('montant', 0)} ({credit.get('libelle', 'N/A')})\\n"
                    
                    entries_context += "\\n"
                
                messages.append({"role": "assistant", "content": entries_context})
            
            # Ajouter la demande actuelle
            messages.append({"role": "user", "content": prompt})
            
            # Streaming avec support des outils
            stream = self.client.chat.completions.create(
                model="gpt-4o-2024-08-06",
                messages=messages,
                tools=self.tool_system.get_openai_function_definitions(),
                tool_choice="auto",
                temperature=0.7,
                max_tokens=1500,
                stream=True
            )
            
            # Variables pour g√©rer les appels d'outils en streaming
            current_tool_calls = {}
            accumulated_content = ""
            
            for chunk in stream:
                if chunk.choices[0].delta.content:
                    # Contenu de r√©ponse normale
                    content = chunk.choices[0].delta.content
                    accumulated_content += content
                    yield content
                
                elif chunk.choices[0].delta.tool_calls:
                    # Le LLM veut utiliser un outil
                    for tool_call_delta in chunk.choices[0].delta.tool_calls:
                        if tool_call_delta.id:
                            # D√©but d'un nouvel appel d'outil
                            current_tool_calls[tool_call_delta.id] = {
                                'function': {
                                    'name': tool_call_delta.function.name if tool_call_delta.function.name else '',
                                    'arguments': tool_call_delta.function.arguments if tool_call_delta.function.arguments else ''
                                }
                            }
                        else:
                            # Continuation des arguments de l'outil
                            for call_id, call_data in current_tool_calls.items():
                                if tool_call_delta.function.arguments:
                                    call_data['function']['arguments'] += tool_call_delta.function.arguments
                
                elif chunk.choices[0].finish_reason == "tool_calls":
                    # Le LLM a termin√© les appels d'outils, les ex√©cuter
                    yield "\\n\\nüîß *Ex√©cution des calculs...*\\n\\n"
                    
                    tool_results = []
                    for call_id, call_data in current_tool_calls.items():
                        function_name = call_data['function']['name']
                        function_args = json.loads(call_data['function']['arguments'])
                        
                        # Ex√©cuter l'outil
                        tool_result = self.tool_system.execute_tool(function_name, function_args)
                        tool_results.append(tool_result)
                        
                        # Streamer le r√©sultat du calcul
                        if tool_result.get('success'):
                            yield f"‚úÖ **{function_name}**: {tool_result.get('formatted_result', 'Calcul effectu√©')}\\n\\n"
                        else:
                            yield f"‚ùå **Erreur de calcul**: {tool_result.get('error', 'Erreur inconnue')}\\n\\n"
                    
                    # Maintenant demander au LLM de formuler une r√©ponse finale
                    yield "üìù *Formulation de la r√©ponse...*\\n\\n"
                    
                    # Ajouter les r√©sultats des outils au contexte
                    for i, (call_id, call_data) in enumerate(current_tool_calls.items()):
                        messages.append({
                            "tool_call_id": call_id,
                            "role": "tool", 
                            "name": call_data['function']['name'],
                            "content": json.dumps(tool_results[i])
                        })
                    
                    # Demander la r√©ponse finale en streaming
                    final_stream = self.client.chat.completions.create(
                        model="gpt-4o-2024-08-06",
                        messages=messages,
                        temperature=0.7,
                        max_tokens=1000,
                        stream=True
                    )
                    
                    for final_chunk in final_stream:
                        if final_chunk.choices[0].delta.content:
                            yield final_chunk.choices[0].delta.content
                    
                    break
                
        except Exception as e:
            yield f"\\n\\n‚ùå Erreur lors du streaming: {str(e)}"
            # Appel OpenAI en mode stream
            try:
                response = self.client.chat.completions.create(
                    model="gpt-4o-2024-08-06",
                    messages=messages,
                    temperature=0.7,
                    max_tokens=1000,
                    top_p=1.0,
                    frequency_penalty=0.0,
                    presence_penalty=0.0,
                    stream=True
                )
                first_chunk = True
                for chunk in response:
                    content = chunk.choices[0].delta.content if chunk.choices[0].delta else ""
                    if content:
                        # Ajouter le user_greeting au tout d√©but
                        if first_chunk and user_greeting:
                            yield user_greeting + content
                            first_chunk = False
                        else:
                            yield content
            except Exception as api_error:
                yield f"[ERREUR LLM: {str(api_error)}]"
        except Exception as e:
            yield f"[ERREUR AGENT: {str(e)}]"

    def get_account_summary(self, account, start_date=None, end_date=None):
        """
        G√©n√®re un r√©sum√© des mouvements d'un compte sp√©cifique.
        """
        try:
            # Filtrer les √©critures par compte
            query = Q()
            
            # Ajouter le compte au d√©bit
            query |= Q(debit_data__contains=[{"compte": account}])
            
            # Ajouter le compte au cr√©dit
            query |= Q(credit_data__contains=[{"compte": account}])
            
            entries = JournalEntry.objects.filter(query)
            
            # Filtre par date si sp√©cifi√©
            if start_date:
                entries = entries.filter(date__gte=start_date)
            if end_date:
                entries = entries.filter(date__lte=end_date)
            
            # Filtrer par utilisateur si sp√©cifi√©
            if self.user_id:
                entries = entries.filter(Q(created_by_id=self.user_id) | Q(created_by__isnull=True))
            
            # Calculer le total des d√©bits et cr√©dits pour ce compte
            total_debit = 0
            total_credit = 0
            movements = []
            
            for entry in entries:
                # Chercher dans les d√©bits
                debit_amount = 0
                for debit in entry.debit_data:
                    if debit.get("compte") == account:
                        debit_amount += float(debit.get("montant", 0))
                
                # Chercher dans les cr√©dits
                credit_amount = 0
                for credit in entry.credit_data:
                    if credit.get("compte") == account:
                        credit_amount += float(credit.get("montant", 0))
                
                # Ajouter au total
                total_debit += debit_amount
                total_credit += credit_amount
                
                # Ajouter aux mouvements si concern√©
                if debit_amount > 0 or credit_amount > 0:
                    movements.append({
                        "date": entry.date.strftime("%d/%m/%Y"),
                        "description": entry.description,
                        "debit": debit_amount,
                        "credit": credit_amount,
                        "entry_id": f"entry_{entry.id}"
                    })
            
            return {
                "account": account,
                "total_debit": total_debit,
                "total_credit": total_credit,
                "balance": total_debit - total_credit,
                "movements": movements,
                "entry_count": len(movements)
            }
            
        except Exception as e:
            print(f"Erreur lors de la g√©n√©ration du r√©sum√© de compte: {e}")
            return {
                "account": account,
                "error": str(e),
                "entry_count": 0
            }
    
    def _truncate_text(self, text, max_length=100):
        """
        Tronque un texte √† la longueur sp√©cifi√©e.
        """
        if not text:
            return ""
        if len(text) <= max_length:
            return text
        return text[:max_length] + "..."

    def _extract_keywords(self, text: str) -> List[str]:
        """
        Extract potential accounting-related keywords from text.
        
        Args:
            text (str): The text to analyze
            
        Returns:
            List[str]: Extracted keywords
        """
        # Basic keyword extraction - in production use NLP or LLM
        accounting_terms = [
            "facture", "paiement", "achat", "vente", "client", "fournisseur",
            "banque", "caisse", "dette", "cr√©ance", "TVA", "imp√¥t",
            "salaire", "immobilisation", "amortissement", "stock"
        ]
        
        # Pattern pour extraire des comptes SYSCOHADA
        account_pattern = r'\b\d{3,6}\b'  # Format: 3 √† 6 chiffres
        
        # Extract numbers that might be dates, amounts, or reference numbers
        amount_pattern = r'\b\d+(?:[.,]\d+)?\b'
        date_pattern = r'\b\d{1,2}[/-]\d{1,2}[/-]\d{2,4}\b'
        ref_pattern = r'\b[A-Z0-9]{2,}-\d+\b|\b[A-Z0-9]+\d+[A-Z0-9]*\b'
        
        # Combine extracted information
        keywords = []
        
        # Add accounting terms found in the text
        for term in accounting_terms:
            if re.search(r'\b' + re.escape(term) + r'\b', text.lower()):
                keywords.append(term)
        
        # Add account numbers
        accounts = re.findall(account_pattern, text)
        if accounts:
            keywords.extend(accounts)
        
        # Add amounts, dates and references
        amounts = re.findall(amount_pattern, text)
        dates = re.findall(date_pattern, text)
        refs = re.findall(ref_pattern, text)
        
        keywords.extend(amounts + dates + refs)
        
        return list(set(keywords))  # Remove duplicates

    def _format_entry_for_response(self, entry: Dict[str, Any]) -> Dict[str, Any]:
        """
        Format a journal entry for the API response.
        
        Args:
            entry (Dict[str, Any]): The journal entry to format
            
        Returns:
            Dict[str, Any]: The formatted entry
        """
        try:
            formatted = {
                "id": entry.get("id"),
                "date": entry.get("date"),
                "description": entry.get("description"),
                "piece_reference": entry.get("piece_reference"),
                "debit_total": sum(item.get("montant", 0) for item in entry.get("debit", [])),
                "credit_total": sum(item.get("montant", 0) for item in entry.get("credit", [])),
                "debit": [{
                    "compte": item.get("compte", ""),
                    "montant": item.get("montant", 0),
                    "libelle": item.get("libelle", "")
                } for item in entry.get("debit", [])],
                "credit": [{
                    "compte": item.get("compte", ""),
                    "montant": item.get("montant", 0),
                    "libelle": item.get("libelle", "")
                } for item in entry.get("credit", [])]
            }
            return formatted
        except Exception as e:
            print(f"Error formatting entry: {e}")
            return entry  # Return original if formatting fails

    def search_entries(self, search_query, user_only=True):
        """
        Recherche des √©critures comptables √† partir d'une requ√™te en langage naturel.
        Cette m√©thode peut √™tre appel√©e en dehors d'une conversation pour obtenir l'historique.
        
        Args:
            search_query (str): Requ√™te de recherche
            user_only (bool): Si True, limite la recherche aux entr√©es de l'utilisateur actuel
            
        Returns:
            List[Dict]: Liste des √©critures comptables pertinentes
        """
        # Temporairement sauvegarder l'ID utilisateur actuel
        original_user_id = self.user_id
        
        try:
            # Si user_only=False et qu'un utilisateur est d√©fini, on recherche toutes les entr√©es
            if not user_only:
                temp_user_id = self.user_id
                self.user_id = None
            
            # Utiliser la m√©thode existante pour trouver des √©critures
            relevant_entries = self._get_relevant_accounting_entries(search_query, max_entries=10)
            
            # Restaurer l'ID utilisateur si modifi√©
            if not user_only:
                self.user_id = temp_user_id
                
            # Formatter les entr√©es trouv√©es
            formatted_entries = [self._format_entry_for_response(entry) for entry in relevant_entries]
            
            return {
                "query": search_query,
                "entries": formatted_entries,
                "count": len(formatted_entries)
            }
            
        except Exception as e:
            print(f"Erreur lors de la recherche d'√©critures: {e}")
            return {
                "query": search_query,
                "entries": [],
                "count": 0,
                "error": str(e)
            }
        finally:
            # Restaurer l'ID utilisateur original
            self.user_id = original_user_id

    def get_user_entries(self, limit=20):
        """
        R√©cup√®re les √©critures comptables de l'utilisateur actuel.
        Cette m√©thode peut √™tre utilis√©e pour afficher l'historique complet.
        
        Args:
            limit (int): Nombre maximal d'entr√©es √† r√©cup√©rer
            
        Returns:
            List[Dict]: Liste des √©critures comptables de l'utilisateur
        """
        try:
            if not self.user_id:
                return {"error": "Aucun utilisateur sp√©cifi√©", "entries": [], "count": 0}
            
            # R√©cup√©rer les entr√©es de l'utilisateur
            entries = JournalEntry.objects.filter(
                Q(created_by_id=self.user_id) | Q(created_by__isnull=True)
            ).order_by('-date', '-created_at')[:limit]
            
            # Formatter les entr√©es
            formatted_entries = []
            for entry in entries:
                formatted_entries.append({
                    "id": entry.id,
                    "date": entry.date.strftime("%d/%m/%Y"),
                    "piece_reference": entry.piece_reference,
                    "description": entry.description,
                    "debit_total": sum(item.get("montant", 0) for item in entry.debit_data),
                    "credit_total": sum(item.get("montant", 0) for item in entry.credit_data),
                    "debit": entry.debit_data,
                    "credit": entry.credit_data
                })
            
            return {
                "entries": formatted_entries,
                "count": len(formatted_entries)
            }
            
        except Exception as e:
            print(f"Erreur lors de la r√©cup√©ration des √©critures utilisateur: {e}")
            return {"error": str(e), "entries": [], "count": 0}
